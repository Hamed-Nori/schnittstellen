{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "pre > code {\n",
    "    background-color: #3A3960 !important;\n",
    "    padding: 10px;\n",
    "    display: block;\n",
    "    border-radius: 5px;\n",
    "    border: 1px solid #ccc;\n",
    "    overflow-x: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "# Python requests-Modul: Web Scraping\n",
    "\n",
    "Web Scraping ist eine Technik, mit der man automatisiert Daten von Webseiten extrahieren kann. Anstatt manuell Inhalte von einer Webseite zu kopieren, kann ein Web Scraper dies automatisch tun. Dabei ruft ein Programm die HTML-Seite einer Webseite auf, analysiert die Struktur und extrahiert relevante Informationen.\n",
    "\n",
    "**Wie funktioniert Web Scraping?**\n",
    "\n",
    "Der Prozess des Web Scraping läuft in folgenden Schritten ab:\n",
    "1. Anfrage senden – Das Skript sendet eine HTTP-Request (z. B. mit requests.get()) an eine Webseite.\n",
    "2. Antwort erhalten – Die Webseite antwortet mit HTML-Code, JavaScript oder JSON-Daten.\n",
    "3. Daten extrahieren – Der HTML-Code wird mit einer Bibliothek wie BeautifulSoup analysiert.\n",
    "4. Daten speichern – Die extrahierten Daten werden z. B. in einer Datenbank oder CSV-Datei gespeichert.\n",
    "\n",
    "**Wo wird Web Scraping genutzt?**\n",
    "\n",
    "1. Preisvergleich: Automatische Preisüberwachung für E-Commerce-Websites.\n",
    "2. Datenanalyse: Analyse von Nachrichten, Social Media oder Blogs.\n",
    "3. SEO & Marketing: Analyse von Konkurrenz-Websites und Keywords.\n",
    "4. Automatisierte Recherchen: Sammeln von Unternehmensdaten oder Kontaktinformationen.\n",
    "5. Börsen- und Finanzdaten: Automatische Erfassung von Aktienkursen oder Kryptopreisen.\n",
    "\n",
    "## Einführungsbeispiel\n",
    "\n",
    "Wir werden uns am Anfang auf die folgende Website konzentrieren: https://www.scrapethissite.com/pages/forms/\n",
    "<br>\n",
    "<br>\n",
    "Als erstes müssen folgende Module installiert werden:\n",
    "```\n",
    "pip3 install beautifulsoup4\n",
    "pip3 install requests\n",
    "```\n",
    "\n",
    "### Schritt 1: HTML der Webseite abrufen\n",
    "\n",
    "Nun können wir den Inhalt der Website uns ausgeben lassen:\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "URL = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "\n",
    "response  = requests.get(URL)\n",
    "content = response.text\n",
    "print(f\"Response: {content}\")\n",
    "```\n",
    "\n",
    "Durch \"response.text\" wird der HTML-Code der Webseite als String zurück gegeben. Dieser HTML-Code enthält die gesamte Struktur der Seite (z. B. `<html>`, `<head>`, `<body>`, `<div>`, etc.). Der HTML-Code kann sehr lang sein, da er die gesamte Struktur der Webseite enthält. \n",
    "\n",
    "### Schritt 2: HTML mit BeautifulSoup parsen\n",
    "\n",
    "Was kann man nun mit dieser HTML-Antwort machen? Die HTML-Daten sind momentan nur als reiner Text verfügbar. Um sie zu strukturieren und gezielt auszulesen, nutzt man BeautifulSoup:\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "URL = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "\n",
    "response  = requests.get(URL)\n",
    "content = response.text\n",
    "\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "# Mit \"prettify()\" wird der Code schöner ausgegeben\n",
    "print(f\"Response: {soup.prettify()}\")\n",
    "```\n",
    "\n",
    "Der Befehl `BeautifulSoup(content, \"html.parser\")` analysiert den HTML-Code und stellt ihn in einer baumartigen Struktur dar, die einfach durchsucht und bearbeitet werden kann.<br>\n",
    "\"html.parser\": Gibt an, dass der HTML-Code mit Pythons eingebautem HTML-Parser analysiert werden soll. Es gibt jedoch noch viele andere Parses die genutzt werden können.\n",
    "\n",
    "### Schritt 3: Bestimmte Elemente auslesen\n",
    "\n",
    "Nachdem der HTML-Code geparsed wurde, können wir sehr einfach jedes einzelne Element aus der HTML-Struktur ansprechen und auslesen. Wir wollen alle NHL-Teams als Liste haben:\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "URL = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "\n",
    "response  = requests.get(URL)\n",
    "content = response.text\n",
    "\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "teams = soup.find_all(\"tr\", class_=\"team\")\n",
    "print(teams)\n",
    "```\n",
    "\n",
    "Die Methode `find_all()` sucht in der gesamten HTML-Struktur nach allen passenden Elementen:\n",
    "- Der erste Parameter (\"tr\") gibt an, dass alle `<tr>`-Elemente (Tabellenzeilen) durchsucht werden sollen.\n",
    "- Der zweite Parameter (class_=\"team\") schränkt die Suche weiter ein, sodass nur `<tr>`-Elemente mit der CSS-Klasse \"team\" zurückgegeben werden.\n",
    "- Gibt eine Liste von Tag-Objekten zurück, also alle gefundenen tr-Elemente mit der Klasse \"team\".\n",
    "- Mit einem Indexer kann man auf jedes Element der Liste dann zugreifen z.B. `print(teams[0])`\n",
    "\n",
    "Jetzt möchten wir in jedem Tag-Objekt nach `<tr>`-Elementen suchen, mit der Klasse \"team\":\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "URL = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "\n",
    "response  = requests.get(URL)\n",
    "content = response.text\n",
    "\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "teams = soup.find_all(\"tr\", class_=\"team\")\n",
    "\n",
    "for team in teams:\n",
    "    team_name = team.find(\"td\", class_=\"name\").text.strip()\n",
    "    print(f\"Team: {team_name}\")\n",
    "```\n",
    "\n",
    "Die Methode `find()` wird verwendet, um das erste gefundene HTML-Element zu suchen, das bestimmten Kriterien entspricht: `element = soup.find(\"tag_name\", attributes)`\n",
    "- tag_name: Der HTML-Tag, den du suchst (z. B. \"td\", \"div\", \"span\").\n",
    "- attributes: Ein optionales Dictionary oder benannte Parameter zur Filterung (z. B. class_=\"name\").\n",
    "\n",
    "Das Attribut \"text\" und die Methode \"strip()\":\n",
    "- .text extrahiert den Textinhalt aus dem `<td>`-Element.\n",
    "- .strip() entfernt überflüssige Leerzeichen am Anfang und Ende.\n",
    "\n",
    "Unterschied zwischen find() und find_all():\n",
    "- find(): Gibt das erste gefundene Element zurück (oder None, falls nicht vorhanden)\n",
    "- find_all(): Gibt eine Liste aller gefundenen Elemente zurück\n",
    "\n",
    "### Schritt 4: Mehrere Datenfelder extrahieren\n",
    "\n",
    "Neben den Teamnamen möchten wir weitere Informationen sammeln:\n",
    "- Jahr (year)\n",
    "- Siege (wins)\n",
    "- Niederlagen (losses)\n",
    "- Tore geschossen (GF) (gf)\n",
    "- Tore kassiert (GA) (ga)\n",
    "- Tor-Differenz (diff)\n",
    "\n",
    "Um mit den Daten besser umzugehen verwenden wir pandas:\n",
    "\n",
    "```\n",
    "pip3 install pandas\n",
    "```\n",
    "\n",
    "Jetzt können wir mit sehr einfachen Methoden viele Daten extrahieren und in ein DataFrame speichern:\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "URL = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "\n",
    "response  = requests.get(URL)\n",
    "content = response.text\n",
    "\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "teams = soup.find_all(\"tr\", class_=\"team\")\n",
    "data = []\n",
    "\n",
    "for team in teams:\n",
    "    team_name = team.find(\"td\", class_=\"name\").text.strip()\n",
    "    year = team.find(\"td\", class_=\"year\").text.strip()\n",
    "    wins = team.find(\"td\", class_=\"wins\").text.strip()\n",
    "    losses = team.find(\"td\", class_=\"losses\").text.strip()\n",
    "    goals_for = team.find(\"td\", class_=\"gf\").text.strip()\n",
    "    goals_against = team.find(\"td\", class_=\"ga\").text.strip()\n",
    "    diff = team.find(\"td\", class_=\"diff\").text.strip()\n",
    "\n",
    "    data.append([team_name, year, wins, losses, goals_for, goals_against, diff])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Team\", \"Jahr\", \"Siege\", \"Niederlagen\", \"GF\", \"GA\", \"+/-\"])\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### Schritt 5: Speichern als CSV\n",
    "\n",
    "```python\n",
    "df.to_csv(\"nhl_teams.csv\", index=False)\n",
    "```\n",
    "\n",
    "### Schritt 6: Umgang mit Pagination\n",
    "\n",
    "Viele Webseiten zeigen nur eine begrenzte Anzahl von Daten pro Seite und nutzen Pagination (Seiten-Navigation), um mehr Inhalte bereitzustellen. In unserem Fall gibt es mehrere Seiten mit NHL-Teams, die über die `?page_num=`-Parameter in der URL aufgerufen werden. Die Links zur nächsten Seite folgen diesem Muster:\n",
    "\n",
    "```\n",
    "https://www.scrapethissite.com/pages/forms/?page_num=1\n",
    "https://www.scrapethissite.com/pages/forms/?page_num=2\n",
    "https://www.scrapethissite.com/pages/forms/?page_num=3\n",
    "...\n",
    "```\n",
    "\n",
    "Die Lösung: Man muss eine Schleife erstellen welche jede Seite durchläuft.\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "BASE_URL = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "PAGE_PARAM = \"?page_num=\"\n",
    "\n",
    "all_teams_data = []\n",
    "\n",
    "for page_num in range(1, 25):\n",
    "    print(f\"On page {page_num}\")\n",
    "\n",
    "    url = BASE_URL + PAGE_PARAM + str(page_num)\n",
    "    response  = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Fehler beim Laden von Seite {page_num}\")\n",
    "        break\n",
    "\n",
    "    content = response.text\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    teams = soup.find_all(\"tr\", class_=\"team\")\n",
    "\n",
    "    for team in teams:\n",
    "        team_name = team.find(\"td\", class_=\"name\").text.strip()\n",
    "        year = team.find(\"td\", class_=\"year\").text.strip()\n",
    "        wins = team.find(\"td\", class_=\"wins\").text.strip()\n",
    "        losses = team.find(\"td\", class_=\"losses\").text.strip()\n",
    "        goals_for = team.find(\"td\", class_=\"gf\").text.strip()\n",
    "        goals_against = team.find(\"td\", class_=\"ga\").text.strip()\n",
    "        diff = team.find(\"td\", class_=\"diff\").text.strip()\n",
    "\n",
    "        all_teams_data.append([team_name, year, wins, losses, goals_for, goals_against, diff])\n",
    "\n",
    "df = pd.DataFrame(all_teams_data, columns=[\"Team\", \"Jahr\", \"Siege\", \"Niederlagen\", \"GF\", \"GA\", \"+/-\"])\n",
    "df.to_csv(\"nhl_teams_all_pages.csv\", index=False)\n",
    "print(df)\n",
    "```\n",
    "\n",
    "Einen Nachteil hat der Code, die Anzahl der Seiten ist statisch `for page_num in range(1, 25)`. Dieses Problem können wir jedoch lösen:\n",
    "-  Methode 1: Die letzte Seitenzahl aus der Paginierung auslesen\n",
    "-  Methode 2: Prüfen, ob eine Seite existiert\n",
    "\n",
    "**Methode 1:**\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "BASE_URL = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "PAGE_PARAM = \"?page_num=\"\n",
    "\n",
    "all_teams_data = []\n",
    "page_numbers = []\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "if response.status_code != 200:\n",
    "    print(\"Fehler beim Laden!\")\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "pagination_links = soup.select(\"ul.pagination a\")\n",
    "\n",
    "for link in pagination_links:\n",
    "    number_str = link.text.strip()\n",
    "    if number_str.isdigit(): \n",
    "        page_numbers.append(int(number_str))\n",
    "\n",
    "if page_numbers:\n",
    "    max_page = max(page_numbers)\n",
    "else:\n",
    "    max_page = 1\n",
    "\n",
    "print(f\"Die letzte verfügbare Seite ist: {max_page}\")\n",
    "\n",
    "all_teams_data = []\n",
    "\n",
    "for page_num in range(1, max_page):\n",
    "    print(f\"On page {page_num}\")\n",
    "\n",
    "    url = BASE_URL + PAGE_PARAM + str(page_num)\n",
    "    response  = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Fehler beim Laden von Seite {page_num}\")\n",
    "        break\n",
    "\n",
    "    content = response.text\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    teams = soup.find_all(\"tr\", class_=\"team\")\n",
    "\n",
    "    for team in teams:\n",
    "        team_name = team.find(\"td\", class_=\"name\").text.strip()\n",
    "        year = team.find(\"td\", class_=\"year\").text.strip()\n",
    "        wins = team.find(\"td\", class_=\"wins\").text.strip()\n",
    "        losses = team.find(\"td\", class_=\"losses\").text.strip()\n",
    "        goals_for = team.find(\"td\", class_=\"gf\").text.strip()\n",
    "        goals_against = team.find(\"td\", class_=\"ga\").text.strip()\n",
    "        diff = team.find(\"td\", class_=\"diff\").text.strip()\n",
    "\n",
    "        all_teams_data.append([team_name, year, wins, losses, goals_for, goals_against, diff])\n",
    "\n",
    "df = pd.DataFrame(all_teams_data, columns=[\"Team\", \"Jahr\", \"Siege\", \"Niederlagen\", \"GF\", \"GA\", \"+/-\"])\n",
    "df.to_csv(\"nhl_teams_all_pages.csv\", index=False)\n",
    "print(df)\n",
    "```\n",
    "\n",
    "**Methode 2:**\n",
    "\n",
    "Ist leider in dem Fall nicht möglich, da solche Seiten wie z.B `https://www.scrapethissite.com/pages/forms/?page_num=29` vorhanden sind, jedoch ohne Daten.\n",
    "\n",
    "**Code umstrukturieren:**\n",
    "\n",
    "- Der Code soll in Funktionen unterteilt sein\n",
    "- Die Datentypen sollen klar und deutlich erkennbar sein\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "BASE_URL: str = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "PAGE_PARAM: str = \"?page_num=\"\n",
    "\n",
    "def get_last_page() -> int:\n",
    "    \"\"\"\n",
    "    Findet die letzte verfügbare Seite anhand der Paginierung.\n",
    "    Falls keine Paginierung vorhanden ist oder ein Fehler auftritt, wird die Standardseite 1 zurückgegeben.\n",
    "    \n",
    "    Returns:\n",
    "        int: Die höchste Seitenzahl oder 1, falls keine Paginierung existiert.\n",
    "    \"\"\"\n",
    "    response: requests.Response = requests.get(BASE_URL)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Fehler beim Laden der Hauptseite!\")\n",
    "        return 1  \n",
    "\n",
    "    soup: BeautifulSoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    pagination_links = soup.select(\"ul.pagination a\")\n",
    "\n",
    "    page_numbers: list[int] = [int(link.text.strip()) for link in pagination_links if link.text.strip().isdigit()]\n",
    "    \n",
    "    return max(page_numbers) if page_numbers else 1\n",
    "\n",
    "def scrape_team_data() -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Ruft die Daten aller Seiten ab und gibt sie als DataFrame zurück.\n",
    "    \n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: Enthält die Team-Daten, falls erfolgreich, sonst None.\n",
    "    \"\"\"\n",
    "    last_page: int = get_last_page()\n",
    "    all_teams_data: list[list[str]] = []\n",
    "\n",
    "    for page_num in range(1, last_page + 1):\n",
    "        print(f\"Lade Daten von Seite {page_num}...\")\n",
    "        \n",
    "        url: str = BASE_URL + PAGE_PARAM + str(page_num)\n",
    "        response: requests.Response = requests.get(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Fehler beim Laden von Seite {page_num}\")\n",
    "            continue\n",
    "\n",
    "        soup: BeautifulSoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        teams = soup.find_all(\"tr\", class_=\"team\")\n",
    "\n",
    "        for team in teams:\n",
    "            team_name: str = team.find(\"td\", class_=\"name\").text.strip()\n",
    "            year: str = team.find(\"td\", class_=\"year\").text.strip()\n",
    "            wins: str = team.find(\"td\", class_=\"wins\").text.strip()\n",
    "            losses: str = team.find(\"td\", class_=\"losses\").text.strip()\n",
    "            goals_for: str = team.find(\"td\", class_=\"gf\").text.strip()\n",
    "            goals_against: str = team.find(\"td\", class_=\"ga\").text.strip()\n",
    "            diff: str = team.find(\"td\", class_=\"diff\").text.strip()\n",
    "\n",
    "            all_teams_data.append([team_name, year, wins, losses, goals_for, goals_against, diff])\n",
    "\n",
    "    if all_teams_data:\n",
    "        df: pd.DataFrame = pd.DataFrame(all_teams_data, columns=[\"Team\", \"Jahr\", \"Siege\", \"Niederlagen\", \"GF\", \"GA\", \"+/-\"])\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Keine Daten gefunden!\")\n",
    "        return None\n",
    "\n",
    "def save_data_to_csv(df: pd.DataFrame, filename: str = \"nhl_teams_all_pages.csv\") -> None:\n",
    "    \"\"\"\n",
    "    Speichert den DataFrame als CSV-Datei.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Der DataFrame, der gespeichert werden soll.\n",
    "        filename (str): Der Name der Datei (Standard: 'nhl_teams_all_pages.csv').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if df is not None:\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"\\nDaten erfolgreich gespeichert als: {filename}\")\n",
    "    else:\n",
    "        print(\"Keine Daten zum Speichern vorhanden!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df: Optional[pd.DataFrame] = scrape_team_data()\n",
    "    save_data_to_csv(df)\n",
    "```\n",
    "\n",
    "**Explorative Datenanalyse:**\n",
    "\n",
    "Jetzt können wir mit den Daten eine einfache explorative Datenanalyse betreiben und interessante Einblicke bekommen:\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE_URL: str = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "PAGE_PARAM: str = \"?page_num=\"\n",
    "\n",
    "def get_last_page() -> int:\n",
    "    \"\"\"\n",
    "    Findet die letzte verfügbare Seite anhand der Paginierung.\n",
    "    Falls keine Paginierung vorhanden ist oder ein Fehler auftritt, wird die Standardseite 1 zurückgegeben.\n",
    "    \n",
    "    Returns:\n",
    "        int: Die höchste Seitenzahl oder 1, falls keine Paginierung existiert.\n",
    "    \"\"\"\n",
    "    response: requests.Response = requests.get(BASE_URL)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Fehler beim Laden der Hauptseite!\")\n",
    "        return 1  \n",
    "\n",
    "    soup: BeautifulSoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    pagination_links = soup.select(\"ul.pagination a\")\n",
    "\n",
    "    page_numbers: list[int] = [int(link.text.strip()) for link in pagination_links if link.text.strip().isdigit()]\n",
    "    \n",
    "    return max(page_numbers) if page_numbers else 1\n",
    "\n",
    "def scrape_team_data() -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Ruft die Daten aller Seiten ab und gibt sie als DataFrame zurück.\n",
    "    \n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: Enthält die Team-Daten, falls erfolgreich, sonst None.\n",
    "    \"\"\"\n",
    "    last_page: int = get_last_page()\n",
    "    all_teams_data: list[list[str]] = []\n",
    "\n",
    "    for page_num in range(1, last_page + 1):\n",
    "        print(f\"Lade Daten von Seite {page_num}...\")\n",
    "        \n",
    "        url: str = BASE_URL + PAGE_PARAM + str(page_num)\n",
    "        response: requests.Response = requests.get(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Fehler beim Laden von Seite {page_num}\")\n",
    "            continue\n",
    "\n",
    "        soup: BeautifulSoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        teams = soup.find_all(\"tr\", class_=\"team\")\n",
    "\n",
    "        for team in teams:\n",
    "            team_name: str = team.find(\"td\", class_=\"name\").text.strip()\n",
    "            year: str = team.find(\"td\", class_=\"year\").text.strip()\n",
    "            wins: str = team.find(\"td\", class_=\"wins\").text.strip()\n",
    "            losses: str = team.find(\"td\", class_=\"losses\").text.strip()\n",
    "            goals_for: str = team.find(\"td\", class_=\"gf\").text.strip()\n",
    "            goals_against: str = team.find(\"td\", class_=\"ga\").text.strip()\n",
    "            diff: str = team.find(\"td\", class_=\"diff\").text.strip()\n",
    "\n",
    "            all_teams_data.append([team_name, year, wins, losses, goals_for, goals_against, diff])\n",
    "\n",
    "    if all_teams_data:\n",
    "        df: pd.DataFrame = pd.DataFrame(all_teams_data, columns=[\"Team\", \"Jahr\", \"Siege\", \"Niederlagen\", \"GF\", \"GA\", \"+/-\"])\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Keine Daten gefunden!\")\n",
    "        return None\n",
    "\n",
    "def save_data_to_csv(df: pd.DataFrame, filename: str = \"nhl_teams_all_pages.csv\") -> None:\n",
    "    \"\"\"\n",
    "    Speichert den DataFrame als CSV-Datei.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Der DataFrame, der gespeichert werden soll.\n",
    "        filename (str): Der Name der Datei (Standard: 'nhl_teams_all_pages.csv').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if df is not None:\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"\\nDaten erfolgreich gespeichert als: {filename}\")\n",
    "    else:\n",
    "        print(\"Keine Daten zum Speichern vorhanden!\")\n",
    "\n",
    "# Ab hier Datenanalyse\n",
    "\n",
    "def load_data(file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Lädt die CSV-Datei und gibt einen DataFrame zurück.\"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    return df\n",
    "\n",
    "def avg_wins_losses_per_year(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Zeigt die durchschnittlichen Siege und Niederlagen pro Jahr an.\"\"\"\n",
    "    yearly_avg = df.groupby(\"Jahr\")[[\"Siege\", \"Niederlagen\"]].mean()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    ax.plot(yearly_avg.index, yearly_avg[\"Siege\"], marker='o', label=\"Durchschnittliche Siege\")\n",
    "    ax.plot(yearly_avg.index, yearly_avg[\"Niederlagen\"], marker='o', label=\"Durchschnittliche Niederlagen\")\n",
    "    \n",
    "    ax.set_title(\"Durchschnittliche Siege und Niederlagen pro Jahr\")\n",
    "    ax.set_xlabel(\"Jahr\")\n",
    "    ax.set_ylabel(\"Anzahl\")\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # df: Optional[pd.DataFrame] = scrape_team_data()\n",
    "    # save_data_to_csv(df)\n",
    "    data = load_data(\"nhl_teams_all_pages.csv\")\n",
    "    print(data)\n",
    "    avg_wins_losses_per_year(data)\n",
    "```\n",
    "\n",
    "Ein schönes User-Interface mit streamlit kann das Projekt angenehmer gestallten:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
